'''
multiprocessing refs: 
https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/3_multiprocessing.ipynb#scrollTo=AvO5BGrVv2Rk
https://subscription.packtpub.com/book/programming/9781839210686/16/ch16lvl1sec38/vectorized-environments
https://www.youtube.com/watch?v=PxoG0A2QoFs&t=778s&ab_channel=ClarityCoders


callbacks:
https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/4_callbacks_hyperparameter_tuning.ipynb
https://www.youtube.com/watch?v=dLP-2Y6yu70&t=12s&ab_channel=sentdex
https://www.youtube.com/watch?v=dLP-2Y6yu70&t=12s&ab_channel=sentdex
    16:26

CALLBACK FUNCTION FROM: https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/4_callbacks_hyperparameter_tuning.ipynb#scrollTo=nzMHj7r3h78m&line=1&uniqifier=1


https://grid2op.readthedocs.io/en/latest/gym.html#default-action-space

https://www.gymlibrary.dev/content/vectorising/

https://pytorch.org/rl/tutorials/multiagent_ppo.html

PIPE ERROR:
The error seems to be related to the observation space format of your environment when setting up the PPO algorithm. 
The NotImplementedError suggests that the Box(0, 255, (240, 256, 1), uint8) observation space is not supported.
'''