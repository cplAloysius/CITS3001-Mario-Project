diff --git a/mpPPO2.py b/mpPPO2.py
index 77e733d..35faa41 100644
--- a/mpPPO2.py
+++ b/mpPPO2.py
@@ -20,6 +20,7 @@ from torch.distributions import Categorical
 from torchrl.objectives import PPOLoss, ClipPPOLoss
 from torchrl.objectives.value.functional import generalized_advantage_estimate
 from torchrl.envs.libs.gym import _get_envs                                                         #https://pytorch.org/rl/tutorials/torchrl_envs.html
+from labml.utils.pytorch import get_modules
 
 from PIL import Image
 import torch.multiprocessing as mp
@@ -36,14 +37,14 @@ import warnings
 warnings.filterwarnings("ignore")
 
 config_dict = {
-    'check_repo_dirty': True,
+    'check_repo_dirty': False,
     'data_path': 'data',
     'experiments_path': 'logs',
     'analytics_path': 'analytics',
-    'web_api': 'TOKEN from app.labml.ai',
-    'web_api_frequency': 60,
-    'web_api_verify_connection': True,
-    'web_api_open_browser': True,
+    'web_api': None,
+    'web_api_frequency': None,
+    'web_api_verify_connection': False,
+    'web_api_open_browser': False,
     'indicators': [
         {
             'class_name': 'Scalar',
@@ -55,8 +56,6 @@ config_dict = {
 
 lab.configure(configurations=config_dict)
 
-print('break')
-
 if torch.cuda.is_available():
     device = torch.device("cuda:1")
 else:
@@ -66,7 +65,6 @@ LEARNING_RATE = 0.000001
 
 class Round:
     def __init__(self):
-        print("IN ROUND")
         self.env = gym_super_mario_bros.make('SuperMarioBros-v3', apply_api_compatibility=True, render_mode='human')
         self.env = JoypadSpace(self.env, SIMPLE_MOVEMENT)
         # self.env = GrayScaleObservation(self.env, keep_dim=True)
@@ -113,17 +111,16 @@ class Round:
     def process_obs(self, obs):
         obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)
         obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)
-        print("PROCESSED OBS")
         return obs
 
 
 def worker_process(remote: multiprocessing.connection.Connection):
     round = Round()
-    print("AFTER ROUND")
 
     while True:
         cmd, data = remote.recv()
         if cmd == "step":
+            print("stepping")
             remote.send(round.fourSteps(data))  
         elif cmd == "reset":
             remote.send(round.reset())
@@ -204,7 +201,6 @@ class Main:
             worker.child.send(("reset", None))
         for i, worker in enumerate(self.workers):
                 self.obs[i] = worker.child.recv()
-                print("got thru")
 
         self.model = MarioModel().to(device) # laptop = CPU, PC = GPU
 
@@ -214,7 +210,7 @@ class Main:
         # store data for every step taken by each worker in parallel
         rewards = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
         actions = np.zeros((self.n_workers, self.worker_steps), dtype=np.int32)
-        done = np.zeros((self.n_workers, self.worker_steps), dtype=np.bool)
+        done = np.zeros((self.n_workers, self.worker_steps), dtype=bool)
         obs = np.zeros((self.n_workers, self.worker_steps, 4, 84, 84), dtype=np.uint8)
         log_pis = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
         values = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
@@ -226,38 +222,42 @@ class Main:
             with torch.no_grad(): # don't compute gradients 
                 obs[:, t] = self.obs # tracks observation from each worker for the model to sample
 
-                # sample actions and store data                                                                                     <- might need to change to GPU for PC
+                # sample actions and store data
                 pi, v = self.model(obs_to_torch(self.obs))
                 values[:, t] = v.cpu().numpy()
                 a = pi.sample()
                 actions[:, t] = a.cpu().numpy()
                 log_pis[:, t] = pi.log_prob(a).cpu().numpy()
 
-                # perform sampled actions on each worker
-                for w, worker in enumerate(self.workers):
-                    worker.child.send(("step", actions[w, t]))
-                for w, worker in enumerate(self.workers):
-                    self.obs[w], rewards[w, t], done[w, t], info = worker.child.recv()                                              # <- might need 5 values here
-                    tracker.add('reward', info['reward'])                                                                           # <- might want to track more values
-
-                advantages = self._calc_advantages(done, rewards, values)
-                samples = {
-                    'obs': obs,
-                    'actions': actions,
-                    'values': values,
-                    'log_pis': log_pis,
-                    'advantages': advantages
-                }
-
-                samples_flat = {}
-                for k, v in samples.items():
-                    v = v.reshape(v.shape[0] * v.shape[1], *v.shape[2:])
-                    if k == 'obs':
-                        samples_flat[k] = obs_to_torch(v)
-                    else:
-                        samples_flat[k] = torch.tensor(v, device=device)
-
-                return samples_flat
+            # perform sampled actions on each worker
+            for w, worker in enumerate(self.workers):
+                worker.child.send(("step", actions[w, t]))
+            for w, worker in enumerate(self.workers):
+                self.obs[w], rewards[w, t], done[w, t], info = worker.child.recv()
+                if info is not None:
+                    tracker.add('reward', info['reward'])
+                    print("info not none")
+                else:
+                    print("info is none")                                                                           
+
+            advantages = self._calc_advantages(done, rewards, values)
+            samples = {
+                'obs': obs,
+                'actions': actions,
+                'values': values,
+                'log_pis': log_pis,
+                'advantages': advantages
+            }
+
+            samples_flat = {}
+            for k, v in samples.items():
+                v = v.reshape(v.shape[0] * v.shape[1], *v.shape[2:])
+                if k == 'obs':
+                    samples_flat[k] = obs_to_torch(v)
+                else:
+                    samples_flat[k] = torch.tensor(v, device=device)
+
+            return samples_flat
             
     def _calc_advantages(self, done: np.ndarray, rewards: np.ndarray, values: np.ndarray) -> np.ndarray:
         # Ensure the input numpy arrays are converted to PyTorch tensors and moved to the correct device.