diff --git a/mpPPO2.py b/mpPPO2.py
index 77e733d..58be676 100644
--- a/mpPPO2.py
+++ b/mpPPO2.py
@@ -20,6 +20,7 @@ from torch.distributions import Categorical
 from torchrl.objectives import PPOLoss, ClipPPOLoss
 from torchrl.objectives.value.functional import generalized_advantage_estimate
 from torchrl.envs.libs.gym import _get_envs                                                         #https://pytorch.org/rl/tutorials/torchrl_envs.html
+from labml.utils.pytorch import get_modules
 
 from PIL import Image
 import torch.multiprocessing as mp
@@ -36,14 +37,14 @@ import warnings
 warnings.filterwarnings("ignore")
 
 config_dict = {
-    'check_repo_dirty': True,
+    'check_repo_dirty': False,
     'data_path': 'data',
     'experiments_path': 'logs',
     'analytics_path': 'analytics',
-    'web_api': 'TOKEN from app.labml.ai',
-    'web_api_frequency': 60,
-    'web_api_verify_connection': True,
-    'web_api_open_browser': True,
+    'web_api': None,
+    'web_api_frequency': None,
+    'web_api_verify_connection': False,
+    'web_api_open_browser': False,
     'indicators': [
         {
             'class_name': 'Scalar',
@@ -55,8 +56,6 @@ config_dict = {
 
 lab.configure(configurations=config_dict)
 
-print('break')
-
 if torch.cuda.is_available():
     device = torch.device("cuda:1")
 else:
@@ -66,7 +65,6 @@ LEARNING_RATE = 0.000001
 
 class Round:
     def __init__(self):
-        print("IN ROUND")
         self.env = gym_super_mario_bros.make('SuperMarioBros-v3', apply_api_compatibility=True, render_mode='human')
         self.env = JoypadSpace(self.env, SIMPLE_MOVEMENT)
         # self.env = GrayScaleObservation(self.env, keep_dim=True)
@@ -113,13 +111,11 @@ class Round:
     def process_obs(self, obs):
         obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)
         obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)
-        print("PROCESSED OBS")
         return obs
 
 
 def worker_process(remote: multiprocessing.connection.Connection):
     round = Round()
-    print("AFTER ROUND")
 
     while True:
         cmd, data = remote.recv()
@@ -204,7 +200,6 @@ class Main:
             worker.child.send(("reset", None))
         for i, worker in enumerate(self.workers):
                 self.obs[i] = worker.child.recv()
-                print("got thru")
 
         self.model = MarioModel().to(device) # laptop = CPU, PC = GPU
 
@@ -214,7 +209,7 @@ class Main:
         # store data for every step taken by each worker in parallel
         rewards = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
         actions = np.zeros((self.n_workers, self.worker_steps), dtype=np.int32)
-        done = np.zeros((self.n_workers, self.worker_steps), dtype=np.bool)
+        done = np.zeros((self.n_workers, self.worker_steps), dtype=bool)
         obs = np.zeros((self.n_workers, self.worker_steps, 4, 84, 84), dtype=np.uint8)
         log_pis = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
         values = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
@@ -238,7 +233,11 @@ class Main:
                     worker.child.send(("step", actions[w, t]))
                 for w, worker in enumerate(self.workers):
                     self.obs[w], rewards[w, t], done[w, t], info = worker.child.recv()                                              # <- might need 5 values here
-                    tracker.add('reward', info['reward'])                                                                           # <- might want to track more values
+                    if info is not None:
+                        tracker.add('reward', info['reward'])                                                                       # <- might want to track more values
+                        print("info not none")
+                    else:
+                        print("info is none")                                                                           
 
                 advantages = self._calc_advantages(done, rewards, values)
                 samples = {