diff --git a/mpPPO2.py b/mpPPO2.py
index 77e733d..29c9201 100644
--- a/mpPPO2.py
+++ b/mpPPO2.py
@@ -20,6 +20,7 @@ from torch.distributions import Categorical
 from torchrl.objectives import PPOLoss, ClipPPOLoss
 from torchrl.objectives.value.functional import generalized_advantage_estimate
 from torchrl.envs.libs.gym import _get_envs                                                         #https://pytorch.org/rl/tutorials/torchrl_envs.html
+from labml.utils.pytorch import get_modules
 
 from PIL import Image
 import torch.multiprocessing as mp
@@ -36,14 +37,14 @@ import warnings
 warnings.filterwarnings("ignore")
 
 config_dict = {
-    'check_repo_dirty': True,
+    'check_repo_dirty': False,
     'data_path': 'data',
     'experiments_path': 'logs',
     'analytics_path': 'analytics',
-    'web_api': 'TOKEN from app.labml.ai',
-    'web_api_frequency': 60,
-    'web_api_verify_connection': True,
-    'web_api_open_browser': True,
+    'web_api': None,
+    'web_api_frequency': None,
+    'web_api_verify_connection': False,
+    'web_api_open_browser': False,
     'indicators': [
         {
             'class_name': 'Scalar',
@@ -55,8 +56,6 @@ config_dict = {
 
 lab.configure(configurations=config_dict)
 
-print('break')
-
 if torch.cuda.is_available():
     device = torch.device("cuda:1")
 else:
@@ -66,7 +65,6 @@ LEARNING_RATE = 0.000001
 
 class Round:
     def __init__(self):
-        print("IN ROUND")
         self.env = gym_super_mario_bros.make('SuperMarioBros-v3', apply_api_compatibility=True, render_mode='human')
         self.env = JoypadSpace(self.env, SIMPLE_MOVEMENT)
         # self.env = GrayScaleObservation(self.env, keep_dim=True)
@@ -113,17 +111,16 @@ class Round:
     def process_obs(self, obs):
         obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)
         obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)
-        print("PROCESSED OBS")
         return obs
 
 
 def worker_process(remote: multiprocessing.connection.Connection):
     round = Round()
-    print("AFTER ROUND")
 
     while True:
         cmd, data = remote.recv()
         if cmd == "step":
+            print("stepping")
             remote.send(round.fourSteps(data))  
         elif cmd == "reset":
             remote.send(round.reset())
@@ -170,7 +167,7 @@ class MarioModel(nn.Module):
         h = F.relu(self.lin(h)) # learn and extract features from data
 
         pi = Categorical(logits=self.pi_logits(h)) # derive policy to dictate the probability of selecting various actions
-        value = self.value(h).reshape(-1) # 
+        value = self.value(h).reshape(-1) # expected return value
 
         return pi, value
         
@@ -195,7 +192,7 @@ class Main:
         self.mini_batch_size = self.batch_size // self.n_mini_batch
         assert (self.batch_size % self.n_mini_batch == 0)
 
-        self.workers = [Worker() for i in range(self.n_workers)]
+        self.workers = [Worker() for i in range(4)]
 
         # initialise tensors for observations
         self.obs = np.zeros((self.n_workers, 4, 84, 84), dtype=np.uint8) # {self.obs.shape} is (4, 4, 84, 84)
@@ -203,8 +200,7 @@ class Main:
         for worker in self.workers:
             worker.child.send(("reset", None))
         for i, worker in enumerate(self.workers):
-                self.obs[i] = worker.child.recv()
-                print("got thru")
+            self.obs[i] = worker.child.recv()
 
         self.model = MarioModel().to(device) # laptop = CPU, PC = GPU
 
@@ -214,7 +210,7 @@ class Main:
         # store data for every step taken by each worker in parallel
         rewards = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
         actions = np.zeros((self.n_workers, self.worker_steps), dtype=np.int32)
-        done = np.zeros((self.n_workers, self.worker_steps), dtype=np.bool)
+        done = np.zeros((self.n_workers, self.worker_steps), dtype=bool)
         obs = np.zeros((self.n_workers, self.worker_steps, 4, 84, 84), dtype=np.uint8)
         log_pis = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
         values = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
@@ -226,62 +222,62 @@ class Main:
             with torch.no_grad(): # don't compute gradients 
                 obs[:, t] = self.obs # tracks observation from each worker for the model to sample
 
-                # sample actions and store data                                                                                     <- might need to change to GPU for PC
+                # sample actions and store data
                 pi, v = self.model(obs_to_torch(self.obs))
                 values[:, t] = v.cpu().numpy()
                 a = pi.sample()
                 actions[:, t] = a.cpu().numpy()
                 log_pis[:, t] = pi.log_prob(a).cpu().numpy()
 
-                # perform sampled actions on each worker
-                for w, worker in enumerate(self.workers):
-                    worker.child.send(("step", actions[w, t]))
-                for w, worker in enumerate(self.workers):
-                    self.obs[w], rewards[w, t], done[w, t], info = worker.child.recv()                                              # <- might need 5 values here
-                    tracker.add('reward', info['reward'])                                                                           # <- might want to track more values
-
-                advantages = self._calc_advantages(done, rewards, values)
-                samples = {
-                    'obs': obs,
-                    'actions': actions,
-                    'values': values,
-                    'log_pis': log_pis,
-                    'advantages': advantages
-                }
-
-                samples_flat = {}
-                for k, v in samples.items():
-                    v = v.reshape(v.shape[0] * v.shape[1], *v.shape[2:])
-                    if k == 'obs':
-                        samples_flat[k] = obs_to_torch(v)
-                    else:
-                        samples_flat[k] = torch.tensor(v, device=device)
-
-                return samples_flat
+            # perform sampled actions on each worker
+            for w, worker in enumerate(self.workers):
+                worker.child.send(("step", actions[w, t]))
+            for w, worker in enumerate(self.workers):
+                self.obs[w], rewards[w, t], done[w, t], info = worker.child.recv()
+                if info is not None:
+                    tracker.add('reward', info['reward'])
+                    print("info not none")
+                else:
+                    print("info is none")                                                                           
+
+            advantages = self._calc_advantages(done, rewards, values)
+            samples = {
+                'obs': obs,
+                'actions': actions,
+                'values': values,
+                'log_pis': log_pis,
+                'advantages': advantages
+            }
+
+            samples_flat = {}
+            for k, v in samples.items():
+                v = v.reshape(v.shape[0] * v.shape[1], *v.shape[2:])
+                if k == 'obs':
+                    samples_flat[k] = obs_to_torch(v)
+                else:
+                    samples_flat[k] = torch.tensor(v, device=device)
+
+            return samples_flat
             
     def _calc_advantages(self, done: np.ndarray, rewards: np.ndarray, values: np.ndarray) -> np.ndarray:
-        # Ensure the input numpy arrays are converted to PyTorch tensors and moved to the correct device.
-        rewards_t = torch.tensor(rewards[..., None], dtype=torch.float32, device=self.device)
-        done_t = torch.tensor(done[..., None], dtype=torch.float32, device=self.device)
-        values_t = torch.tensor(values[..., None], dtype=torch.float32, device=self.device)
-
-        # Assume that the last observed state is the next state after the last state in the trajectory.
-        _, last_value_t = self.model(obs_to_torch(self.obs))
-        last_value_t = last_value_t.squeeze(-1)[..., None]
-
-        # Using generalized_advantage_estimate function
-        next_state_values_t = torch.cat([values_t[..., 1:], last_value_t], dim=-2)
-        advantages, _ = generalized_advantage_estimate(
-            gamma=self.gamma,
-            lmbda=self.lamda,
-            state_value=values_t,
-            next_state_value=next_state_values_t,
-            reward=rewards_t,
-            done=done_t
-        )
-
-        # Convert advantages back to numpy 
-        return advantages.squeeze(-1).cpu().numpy()
+        advantages = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
+        last_advantage = 0
+
+        _, last_value = self.model(obs_to_torch(self.obs))
+        last_value = last_value.cpu().data.numpy()
+
+        for t in reversed(range(self.worker_steps)):
+            mask = 1.0 - done[:, t]
+            last_value = last_value * mask
+            last_advantage = last_advantage * mask
+            delta = rewards[:, t] + self.gamma * last_value - values[:, t]
+
+            last_advantage = delta + self.gamma * self.lamda * last_advantage
+            advantages[:, t] = last_advantage
+
+            last_value = values[:, t]
+
+        return advantages
     
     def train(self, samples: Dict[str, torch.Tensor], learning_rate: float, clip_range: float):
         
@@ -320,6 +316,7 @@ class Main:
         sampled_return = samples['values'] + samples['advantages']
 
         # normalise to stabilise
+        # sampled_normalized_advantage = self._normalize(samples['advantages'])
         sampled_normalized_advantage = self._normalize(samples['advantages'])
 
         # pass obs thru model to get policy and value function
diff --git a/print.py b/print.py
index 0aa7433..fcdc9dc 100644
--- a/print.py
+++ b/print.py
@@ -44,4 +44,31 @@ import labml
 print(labml.__file__)
 print(labml.__package__)
 
-# /Users/oliviamorrison/anaconda3/envs/mario/lib/python3.8/site-packages/labml/__init__.py
\ No newline at end of file
+# /Users/oliviamorrison/anaconda3/envs/mario/lib/python3.8/site-packages/labml/__init__.py
+
+
+'''
+def _calc_advantages(self, done: np.ndarray, rewards: np.ndarray, values: np.ndarray) -> np.ndarray:
+        # Ensure the input numpy arrays are converted to PyTorch tensors and moved to the correct device.
+        rewards_t = torch.tensor(rewards[..., None], dtype=torch.float32, device=device)
+        done_t = torch.tensor(done[..., None], dtype=torch.float32, device=device)
+        values_t = torch.tensor(values[..., None], dtype=torch.float32, device=device)
+
+        # Assume that the last observed state is the next state after the last state in the trajectory.
+        _, last_value_t = self.model(obs_to_torch(self.obs))
+        last_value_t = last_value_t.squeeze(-1)[..., None, None]
+
+        # Using generalized_advantage_estimate function
+        next_state_values_t = torch.cat([values_t[..., 1:], last_value_t], dim=-2) #RuntimeError: Tensors must have same number of dimensions: got 3 and 2
+        advantages, _ = generalized_advantage_estimate(
+            gamma=self.gamma,
+            lmbda=self.lamda,
+            state_value=values_t,
+            next_state_value=next_state_values_t,
+            reward=rewards_t,
+            done=done_t
+        )
+
+        # Convert advantages back to numpy 
+        return advantages.squeeze(-1).cpu().numpy()
+'''
\ No newline at end of file